{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "import urllib\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import re\n",
    "import csv\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_scrap (job_url):\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    options.add_argument('--log-level=3')\n",
    "    driver = webdriver.Chrome(executable_path=\"../code/chromedriver/chromedriver\", options=options)\n",
    "    driver.get(job_url)\n",
    "    sleep(3)\n",
    "    html = driver.page_source\n",
    "    sleep(5)\n",
    "    driver.close()\n",
    "    soup = BeautifulSoup(html, 'lxml', from_encoding=\"utf-8\")\n",
    "    \n",
    "    job_title = []\n",
    "    company = []\n",
    "    job_id = []\n",
    "    seniority = []\n",
    "    job_cat = []\n",
    "    salary = []\n",
    "    requirements = []\n",
    "    post_date = []\n",
    "    emp_type = []\n",
    "    salary_range = []\n",
    "    salary_type = []\n",
    "    experience = []\n",
    "    \n",
    "\n",
    "    for entry in soup.find_all('h1', {'id': \"job_title\"}):\n",
    "        job_title.append(entry.renderContents())\n",
    "    for entry in soup.find_all('p', {'class': \"f6 fw6 mv0 black-80 mr2 di ttu\"}):\n",
    "        company.append(entry.renderContents())\n",
    "    for entry in soup.find_all('span', {'class': \"black-60 db f6 fw4 mv1\"}):\n",
    "        job_id.append(entry.renderContents())     \n",
    "    for entry in soup.find_all('p', {'id': \"employment_type\"}):\n",
    "        emp_type.append(entry.renderContents())  \n",
    "    for entry in soup.find_all('p', {'id': \"seniority\"}):\n",
    "        seniority.append(entry.renderContents())\n",
    "    for entry in soup.find_all('p', {'id': \"job-categories\"}):\n",
    "        job_cat.append(entry.renderContents()) \n",
    "    for entry in soup.find_all('span', {'id': \"last_posted_date\"}):\n",
    "        post_date.append(entry.renderContents())       \n",
    "    for entry in soup.find_all('span', {'class': \"salary_range dib f2-5 fw6 black-80\"}):\n",
    "        salary_range.append(entry.renderContents())       \n",
    "    for entry in soup.find_all('span', {'class': \"salary_type dib f5 fw4 black-60 pr1 i pb\"}):\n",
    "        salary_type.append(entry.renderContents())             \n",
    "    for entry in soup.find_all('p', {'id': \"min_experience\"}):\n",
    "        experience.append(entry.renderContents())            \n",
    " \n",
    "        \n",
    "    salary_all = soup.find_all(\"span\", attrs={'class': 'dib'})\n",
    "    list_of_inner_text = [x.text for x in salary_all]\n",
    "    salary_text = '/ '.join(list_of_inner_text)\n",
    "    salary.append(salary_text)\n",
    "\n",
    "    requirement_all = soup.find(\"div\", attrs={'id': 'description-content'}).findAll('ul')\n",
    "    list_of_inner_text = [x.text for x in requirement_all]\n",
    "    requirements_text = ', '.join(list_of_inner_text)\n",
    "    requirements.append(requirements_text)    \n",
    "    \n",
    "\n",
    "    data = pd.DataFrame(columns = ['Job_Id','Emp_Type','Job_Title','Company','Date_Posted',\n",
    "                                   'Salary_Range','Salary_Type','Year_Experience',\n",
    "                                   'Seniority','Category','Requirements'])\n",
    "\n",
    "    data = data.append({'Job_Id':job_id,'Emp_Type':emp_type,'Job_Title':job_title,'Company':company,\n",
    "                        'Date_Posted':post_date,'Salary_Range':salary_range,\n",
    "                        'Salary_Type':salary_type,'Year_Experience':experience,\n",
    "                        'Seniority':seniority,'Category':job_cat,'Requirements':requirements},ignore_index=True) \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def link_scrap(key_word):\n",
    "    \n",
    "    output_path = '../job_link/'\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])   \n",
    "    options.add_argument('--log-level=3')\n",
    "    driver = webdriver.Chrome(executable_path=\"../code/chromedriver/chromedriver\", options=options)\n",
    "    driver.get(\"https://www.mycareersfuture.sg/\")\n",
    "    elem = driver.find_element_by_name(\"search-text\")\n",
    "    sleep(3)\n",
    "    elem.send_keys(key_word) #job search keyword\n",
    "    sleep(1)\n",
    "    elem.send_keys(Keys.RETURN)\n",
    "    sleep(5)\n",
    "    current_url = driver.current_url\n",
    "    first_page = driver.page_source\n",
    "    sleep(5)\n",
    "    driver.close()\n",
    "\n",
    "    #to extract number of job found\n",
    "    soup = BeautifulSoup(first_page, 'lxml', from_encoding=\"utf-8\")\n",
    "    job_found = soup.find(\"div\", attrs={'data-cy': 'search-result-headers'})\n",
    "    text_list = [text for text in job_found.stripped_strings]\n",
    "    \n",
    "     \n",
    "    while True:\n",
    "        \n",
    "        try:\n",
    "            num_job = int(''.join(list(filter(lambda x: x.isdigit(), text_list[0]))))\n",
    "            print('number of job found:', num_job)\n",
    "            \n",
    "            y=0\n",
    "            links = []\n",
    "            \n",
    "            #20 jobs per page\n",
    "            jobs_per_page = 20\n",
    "            page_num = (num_job + jobs_per_page-1) // jobs_per_page\n",
    "\n",
    "            for page in range(page_num):\n",
    "\n",
    "                new_next_url = current_url.replace('page=' + str(y),'page=' +str(page))\n",
    "                #print(new_next_url)\n",
    "\n",
    "                driver2 = webdriver.Chrome(executable_path=\"../code/chromedriver/chromedriver\")\n",
    "                driver2.get(new_next_url)\n",
    "                sleep(8)\n",
    "                html1 = driver2.page_source \n",
    "                driver2.close()\n",
    "\n",
    "                soup = BeautifulSoup(html1, 'lxml', from_encoding=\"utf-8\")\n",
    "\n",
    "                for entry in soup.find_all(\"a\", attrs={'class': re.compile(r'JobCard')}):\n",
    "                    link = entry.get('href')\n",
    "                    links.append(link)\n",
    "\n",
    "            print('Number of job link extracted:', len(links))\n",
    "\n",
    "            return links\n",
    "            \n",
    "        except ValueError:\n",
    "            print('Job not found, please try with other keyword')\n",
    "            \n",
    "            return 0\n",
    "        \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_crawl(links, key_word):\n",
    " \n",
    "    output_path = '../data_src/'\n",
    "    job_df = pd.DataFrame()\n",
    "\n",
    "    for i in range (len(links)):\n",
    "        job_url  =  \"https://www.mycareersfuture.sg%s\" % links[i]\n",
    "        temp_df = page_scrap(job_url)\n",
    "        job_df = job_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    job_df.to_csv('{}{}_data.csv'.format(output_path,key_word), index=False)\n",
    "        \n",
    "    print('File saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        print('*************************************************************')\n",
    "        print('*                                                           *')\n",
    "        print('*           NUS ISS Group 7 Web-Crawling Program            *')\n",
    "        print('*                                                           *')\n",
    "        print('*************************************************************')\n",
    "        print('\\n')\n",
    "        print('Enter job keyword or type quit to exit')\n",
    "        print('Please re-run program if encounter chrome session error\\n')\n",
    "        \n",
    "        enter = str(input('Please enter: ')).lower()\n",
    "      \n",
    "        if enter == ('quit') or ('exit'):\n",
    "            exit()\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print('Job keyword entered:', enter)\n",
    "            jb_links = link_scrap(enter)\n",
    "            \n",
    "            if jb_links == 0:\n",
    "                print('Job not found, please re-run')\n",
    "                exit()\n",
    "                \n",
    "            else:\n",
    "                job_crawl(jb_links, enter)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************\n",
      "*                                                           *\n",
      "*           NUS ISS Group 7 Web-Crawling Program            *\n",
      "*                                                           *\n",
      "*************************************************************\n",
      "\n",
      "\n",
      "Enter job keyword or type quit to exit\n",
      "Please re-run program if encounter chrome session error\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter:  exit\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
