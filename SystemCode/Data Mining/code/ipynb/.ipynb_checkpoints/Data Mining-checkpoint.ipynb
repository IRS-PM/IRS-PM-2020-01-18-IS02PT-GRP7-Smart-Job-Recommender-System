{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import glob, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import patsy\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from string import printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file(s) loaded: ['../data_src\\\\artificial intelligence_data.csv', '../data_src\\\\Data Science_data.csv', '../data_src\\\\Data Science_data1.csv', '../data_src\\\\Machine Learning_data.csv', '../data_src\\\\Machine Learning_data1.csv']\n"
     ]
    }
   ],
   "source": [
    "src_path = '../data_src/' #web-crawled data source path\n",
    "extension = 'csv' #data source format\n",
    "all_filenames = [i for i in glob.glob('{}*.{}'.format(src_path,extension))] \n",
    "print('Source file(s) loaded:', all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion(src_path, extension):\n",
    "    \n",
    "    all_filenames = [i for i in glob.glob('{}*.{}'.format(src_path,extension))] \n",
    "    print('Source file(s) loaded:', all_filenames)\n",
    "    \n",
    "    #combine all files in the list \n",
    "\n",
    "    df_raw = pd.concat([pd.read_csv(f, encoding='unicode escape',skiprows=0) for f in all_filenames ]) \n",
    "    df_raw.reset_index(inplace=True) \n",
    "    df_raw = df_raw.drop(columns=['index','Unnamed: 0']) \n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(df,title_key,drop_title_key,drop_cat_key):\n",
    "    \n",
    "    #remove duplicate based on Job ID\n",
    "    job_clean = df.drop_duplicates(subset='Job_Id', keep='first')\n",
    "    #drop Salary_Type due to only one unique value 'Monthly'\n",
    "    job_clean = job_clean.drop(columns='Salary_Type')\n",
    "    #remove job without title\n",
    "    job_no_title = job_clean['Job_Title'] == ''\n",
    "    job_clean = job_clean[~job_no_title]\n",
    "    #remove row with all NaN value\n",
    "    job_clean[job_clean.isnull().any(axis=1)]\n",
    "    job_clean = job_clean.dropna()\n",
    "    \n",
    "    #perform data cleaning on every row and columms\n",
    "    clean_list = \"(\\[|\\]|b'|Requirements|'|amp;|xa0|\\\\\\|xe2x80x93|\\\\n|div class=|div class=|span class=|dib|lh-solid|/span|f5-5 i fw4 gray|f5 fw4 ph1|<|>|/div|\\\")\"\n",
    "    for col in job_clean.columns.difference(['Requirements']):\n",
    "        job_clean[col]=job_clean[col].str.replace(clean_list, \"\")\n",
    "\n",
    "    #space remain for Requirements column    \n",
    "    job_clean['Requirements']=job_clean['Requirements'].str.replace(clean_list, \" \")\n",
    "\n",
    "    #remove all non-ascii char except punctuation, digits, ascii_letters and whitespace\n",
    "    job_clean['Requirements'] = job_clean['Requirements'].apply(lambda y: ''.join(filter(lambda x: x in printable, y)))\n",
    "    \n",
    "    #further remove job with same data from all columns\n",
    "    job_clean = job_clean.drop_duplicates(subset=job_clean.columns, keep='first') \n",
    "        \n",
    "    #further filter on job title with specific keywords\n",
    "\n",
    "    key = '|'.join(title_key)\n",
    "    data_job = job_clean['Job_Title'].str.upper().str.contains(key)\n",
    "    job_clean = job_clean[data_job]\n",
    "\n",
    "    #remove job title with unwanted keywords\n",
    "    \n",
    "    key2 = '|'.join(drop_title_key)\n",
    "    non_data_job = job_clean['Job_Title'].str.upper().str.contains(key2)\n",
    "    job_clean = job_clean[~non_data_job]\n",
    "    \n",
    "    #remove job with multiple category\n",
    "    cat_list = \"(/|and)\"\n",
    "    job_clean['Category']=job_clean['Category'].str.replace(cat_list, \",\")\n",
    "    job_clean['Cat_num'] = job_clean['Category'].str.count(',')\n",
    "    \n",
    "    multiple_cat = job_clean['Cat_num']>5\n",
    "    job_clean = job_clean[~multiple_cat]\n",
    "    job_clean = job_clean.drop(columns='Cat_num')\n",
    "    \n",
    "    #remove job with no or multiple seniority\n",
    "    senior_rule = (job_clean['Seniority'].str.count(',')>=1) | (job_clean['Seniority']=='')\n",
    "    job_clean = job_clean[~senior_rule]\n",
    "   \n",
    "    #remove job cat with specific keywords\n",
    "\n",
    "    key3 = '|'.join(drop_cat_key)\n",
    "    rare_cat = job_clean['Category'].str.upper().str.contains(key3)\n",
    "    job_clean = job_clean[~rare_cat]\n",
    "    \n",
    "    #remove row without salary\n",
    "    no_salary = job_clean['Salary_Range'].str.contains('Salary undisclosed')\n",
    "    df_salary = job_clean[~no_salary]\n",
    "    df_no_salary = job_clean[no_salary]\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "    \n",
    "    req_empty = []\n",
    "\n",
    "    for i in range (len(df_salary)):\n",
    "    \n",
    "        if((len(df_salary['Requirements'][i]))<5):\n",
    "            req_empty.append(i)\n",
    "           \n",
    "    #clean & remove row without requirements\n",
    "    df_salary['Requirements']=df_salary['Requirements'].str.replace('(\\n)', \"\")\n",
    "    df_salary = df_salary.drop(req_empty)\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "\n",
    "    return df_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define list of filters\n",
    "\n",
    "title_key = ['DATA', 'MACHINE','ANALYST','MACHINE LEARNING','ANALYTICS', \"SCIENCE\", '4.0','APPLICATION', 'DEEP LEARNING',\n",
    "             'RESEARCH','NLP', 'ARTIFICIAL', \"INTELLIGENT\", 'AI', 'SCIENTIST','SYSTEM','Industry', 'IOT', 'FINANCE', 'FINTECH', \n",
    "             'SOFTWARE', 'ENGINEER', 'ENGINEERING','PROFESSOR','BUSINESS', 'DEVELOPER', 'INDUSTRIAL','AUTOMATION', 'CLOUD',\n",
    "             'SOLUTION','ARCHITECT', 'MANAGER','VP','PRESIDENT', 'TECHNOLOGY', 'SPECIALIST', 'TECHNICAL','LEAD','TECHNOLOGIST']\n",
    "\n",
    "unwanted_title_key = ['PHYSIOTHERAPIST','ACCOUNT','AUDIT','COUNSEL','EXECUTIVE','SALES','GENERAL','MARKET','ELECTRICAL','BUSINESS',\n",
    "                      'ADMIN','CUSTOMER','OFFICER','OPERATION', 'MECHANICAL','CHEMICAL','COORDINATOR','LECTURER','TECHNICIAN']\n",
    "\n",
    "unwanted_cat_key = ['HUMAN','SOCIAL','THERAPY','TAXATION','CUSTOMER','INTERIOR', 'ADMIN','BUILDING', 'SECRETARIAL','INVESTIGATION', \n",
    "                'AUDITING', 'ENVIRONMENT','SALES', 'MARKETING','ADVERTISING','CONSTRUCTION', 'DESIGN','LEGAL','HOSPITALITY',\n",
    "                'PROFESSIONAL']\n",
    "\n",
    "cat_list = ['Information Technology', 'Telecommunications', 'Engineering','Sciences', 'Finance','Healthcare','Management',\n",
    "            'Consulting','Logistics', 'Civil', 'Others']\n",
    "\n",
    "edu_list = ['phd','doctor','master','degree','computer science','engineering','statistic','math','computer engineering',\n",
    "            'business','ph.d']\n",
    "\n",
    "skills_list = ['python','java','scala','hadoop','sql','spark','tensorflow','scikit','linux','pytorch','theano','caffe','Matlab',\n",
    "               'perl','deep learning','nlp','apache','mapreduce','aws','azure','container','kafka','cassandra', 'c\\++' ,'julia',\n",
    "               'jupyter','nltk','tableau','power bi','sas','pandas','git','hive','impala','agile','machine learning','bash',\n",
    "               'natural language','oracle','cloud','flask','golang','optimization','c#','opencv','computer vision','api','jira',\n",
    "               'unix','bash','docker','keras', 'qlik','gcp','scrum', 'airflow','.net','d3.js']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_feature(df, low, med, high):\n",
    "    \n",
    "    #extract salary columns due to contain multiple information\n",
    "    salary_range = df[\"Salary_Range\"].str.split(\"to\", n = 2, expand = True) \n",
    "\n",
    "    #Give columns name to the dataframe\n",
    "    salary_range = salary_range.rename({0:'Min_Salary',1:'Max_Salary'}, axis='columns')\n",
    "\n",
    "    #removed $ and , from salary \n",
    "    for col in salary_range.columns:\n",
    "        salary_range[col]=salary_range[col].str.replace('(\\$|,)', '')\n",
    "\n",
    "    #convert from ojbect to float for statistical infomation\n",
    "    salary_range['Min_Salary'] = salary_range['Min_Salary'].astype('float64')\n",
    "    salary_range['Max_Salary'] = salary_range['Max_Salary'].astype('float64')\n",
    "    \n",
    "    #concat min_max salary dataframe with salary range dataframe\n",
    "    df_salary1 = pd.concat([df, salary_range], axis=1)\n",
    "    df_salary1 = df_salary1.drop(columns='Salary_Range')  \n",
    "    \n",
    "    #create a condition to check for high outliers\n",
    "    abovemean_min = round(10*np.mean(df_salary1['Min_Salary']),0)\n",
    "    abovemean_max = round(10*np.mean(df_salary1['Max_Salary']),0)\n",
    "    \n",
    "    #convert yearly salary into monthly salary\n",
    "\n",
    "    df_salary1['Min_Salary'] = np.where((df_salary1['Min_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Min_Salary']/12),0), df_salary1['Min_Salary'])\n",
    "\n",
    "    df_salary1['Max_Salary'] = np.where((df_salary1['Max_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Max_Salary']/12),0), df_salary1['Max_Salary'])\n",
    "    \n",
    "    #drop unrealistic min and max monthly salary range (which is more than 10 times)\n",
    "    min_max_abnormal = (df_salary1['Max_Salary']>10*df_salary1['Min_Salary'])\n",
    "    df_salary1 = df_salary1[~min_max_abnormal]\n",
    "    \n",
    "    #drop job with max salary less than 2500, assuming data entry/admin/operator job\n",
    "    low_sal = ((df_salary1['Min_Salary']<=1800) | (df_salary1['Max_Salary']<=2500))\n",
    "    df_salary1 = df_salary1[~low_sal]\n",
    "    \n",
    "    #create new feature for average salary\n",
    "    df_salary1['Avg_Salary'] = (df_salary1['Min_Salary'] + df_salary1['Max_Salary']) / 2\n",
    "    \n",
    "    #drop job with outlier salary\n",
    "    salary_outlier = ((df_salary1['Avg_Salary']>20000) | (df_salary1['Avg_Salary']<3000))\n",
    "    df_salary1 = df_salary1[~salary_outlier]\n",
    "    \n",
    "    #bin salary into 3 groups:\n",
    "    #3000 to 4500 - Low\n",
    "    #4500 to 6000 - Med\n",
    "    #6000 and above - High\n",
    "\n",
    "    bins = [low, med, high, np.inf]\n",
    "    names = ['Low', 'Med', 'High']\n",
    "\n",
    "    df_salary1['Salary_range'] = pd.cut(df_salary1['Avg_Salary'], bins, labels=names)\n",
    "    df_salary1 = df_salary1.reset_index(drop=True)\n",
    "    \n",
    "    return df_salary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_type(df):\n",
    "    \n",
    "    #remove others employment type\n",
    "    type_key = ['PART TIME','TEMPORARY','INTERNSHIP','FLEXI','FREELANCE']\n",
    "    key = '|'.join(type_key)\n",
    "    non_type = df['Emp_Type'].str.upper().str.contains(key)\n",
    "    df = df[~non_type]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #consolidate employment type\n",
    "    consolidate = \"(Full Time|Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Permanent\")\n",
    "\n",
    "    consolidate = \"(Contract, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Contract\")\n",
    "\n",
    "    consolidate = \"(Contract, Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Cont_Perm\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_name(df):\n",
    "    \n",
    "    stacked = pd.DataFrame(df['Category'].str.split(',').tolist()).stack()\n",
    "    cat_count = pd.DataFrame(stacked.value_counts(), columns=['Count']).reset_index()\n",
    "    cat_count1 = []\n",
    "\n",
    "    for i in range (len(cat_count)):\n",
    "        cat_count1.append(cat_count['index'][i].lstrip())\n",
    "    \n",
    "    cat_name = list(dict.fromkeys(cat_count1))\n",
    "    \n",
    "    return cat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kie(df):\n",
    "\n",
    "    #extract only number from string\n",
    "    df['Year_Experience'] = df['Year_Experience'].str.extract('(\\d+)')\n",
    "    \n",
    "    #remove comma from cell with string\n",
    "    clean_list = \"(,|;|â||¦|®|)\"\n",
    "    for col in df.columns.difference(['Year_Experience','Min_Salary','Max_Salary','Avg_Salary']):\n",
    "        df[col]=df[col].str.replace(clean_list, \"\")\n",
    "        \n",
    "    #remove extra whitespace between string\n",
    "    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    \n",
    "    #remove row without year of experience\n",
    "    df = df.dropna(subset=['Year_Experience']).reset_index(drop=True)\n",
    "    \n",
    "    #fill NaN in year of experience with 0\n",
    "    #df['Year_Experience'] = df['Year_Experience'].fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_edu_cat(df, cat_list, edu_list):\n",
    "    \n",
    "    for cat in cat_list:\n",
    "        df[cat] = np.where(df['Category'].str.lower().str.contains(cat),1,0)\n",
    "        \n",
    "    for edu in edu_list:\n",
    "        df[edu] = np.where(df['Requirements'].str.lower().str.contains(edu),edu,0) #replace with 1 for machine learning\n",
    "    \n",
    "    sum_elements = [f\"df['{col}']\" for col in edu_list]\n",
    "    to_eval = \"+ '_' + \".join(sum_elements)\n",
    "    df['Qualification'] = eval(to_eval)\n",
    "    clean_list = \"(0|_0|0_|_0_)\"\n",
    "    df['Qualification']=df['Qualification'].str.replace(clean_list, \"\")\n",
    "    df['Qualification']=df['Qualification'].str.replace('_', '%')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_cat(df, skills_list):\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        df[skill] = np.where(df['Requirements'].str.lower().str.contains(skill),skill,0) #replace skill with 1 for machine learning\n",
    "        \n",
    "    sum_elements = [f\"df['{col}']\" for col in skills_list]\n",
    "    to_eval = \"+ '_' + \".join(sum_elements)\n",
    "    df['New_Skills'] = eval(to_eval)\n",
    "    clean_list = \"(0|_0|0_|_0_)\"\n",
    "    df['New_Skills']=df['New_Skills'].str.replace(clean_list, \"\")\n",
    "    df['New_Skills']=df['New_Skills'].str.replace('_', '%')\n",
    "    df['New_Skills'] = df['New_Skills'].str.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count function\n",
    "def word_count(df_col):\n",
    "\n",
    "    str_counts = 0\n",
    "    sum_str = 0\n",
    "\n",
    "    for i in range (len(df_col)):    \n",
    "        str_counts = len(df_col[i].split())\n",
    "        sum_str = sum_str + str_counts\n",
    "\n",
    "    print(sum_str)\n",
    "    \n",
    "#frequent word function\n",
    "def freq_words(word_count, features):\n",
    "\n",
    "    num_word = np.asarray(word_count.sum(axis=0)).reshape(-1)\n",
    "    most_count = num_word.argsort()[::-1]\n",
    "    key_word = pd.Series(num_word[most_count], \n",
    "                           index=features[most_count])\n",
    "\n",
    "    return key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_fil(df):\n",
    "    \n",
    "    #stop words were added to filter some generic recurring business terms.\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['regret','shortlisted', 'candidates','notified','etc', 'take', 'hands','added','able','writting',\n",
    "             'year','years','least', 'related','using', 'and', 'ability','work','skills','advantage','written'\n",
    "            'develop','good','team','design','knowledge','experience','following','areas', 'ability','and','in','to']\n",
    "    \n",
    "    #most common words for requirements\n",
    "    cvt = CountVectorizer(lowercase=True, strip_accents='unicode',max_features=10000, min_df=1, max_df=0.01,\n",
    "                          stop_words=stop, ngram_range=(1,1))\n",
    "    vect_word = cvt.fit_transform(df['Requirements'])\n",
    "    features = np.array(cvt.get_feature_names()) \n",
    "\n",
    "    key_word = freq_words(vect_word, features)\n",
    "    \n",
    "    #update stop_word with common words\n",
    "    new_stop = key_word[key_word<2].index\n",
    "    stop.extend(new_stop)\n",
    "    \n",
    "    #number of word found in Requirements column before clean\n",
    "    print('Number of words before filter:')\n",
    "    word_count(df['Requirements'])\n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    df['Requirements'] = df['Requirements'].str.replace(pat, \" \")\n",
    "    df['Requirements'] = df['Requirements'].map(lambda x: x.strip())\n",
    "    df['Requirements'] = df['Requirements'].replace({' +':\" \"},regex=True)\n",
    "    \n",
    "    print('Number of words after filter:')\n",
    "    word_count(df['Requirements'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output file skills_list\n",
    "def save_file(df, output_path):\n",
    "\n",
    "    df = df.drop(columns=df[skills_list+cat_list+edu_list].columns)\n",
    "    df.to_csv('{}JOB_DATA.csv'.format(output_path), index=False, encoding='utf-8')\n",
    "    print('File saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    src_path = '../data_src/' #web-crawled data source path\n",
    "    extension = 'csv' #data source format\n",
    "#    output_path1 = 'IRS-PM-2020-01-18-IS02PT-GRP7-Smart-Job-Recommender-System/database/' #database output path\n",
    "    output_path1 = 'group7/Database/' #database output path\n",
    "    output_path = '../output/' #result output path\n",
    "\n",
    "    #define salary range\n",
    "    low = 3000\n",
    "    med = 4500\n",
    "    high = 6000\n",
    "    \n",
    "    #data ingestion and data cleaning\n",
    "\n",
    "    df_raw = data_ingestion(src_path, extension)\n",
    "    df = data_clean(df_raw, title_key, unwanted_title_key, unwanted_cat_key)\n",
    "\n",
    "    #feature engineering & data mining\n",
    "    df1 = salary_feature(df, low, med, high)\n",
    "    df1 = emp_type(df1)\n",
    "    df1 = clean_kie(df1)\n",
    "    df1 = job_edu_cat(df1, cat_list, edu_list)\n",
    "    df1 = skill_cat(df1, skills_list)\n",
    "    data_df = stop_word_fil(df1)\n",
    "\n",
    "    #save job data\n",
    "    save_file(data_df, output_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file(s) loaded: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-ab0cf01ca161>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#data ingestion and data cleaning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdf_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_ingestion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munwanted_title_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munwanted_cat_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-54b5c966148b>\u001b[0m in \u001b[0;36mdata_ingestion\u001b[1;34m(src_path, extension)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#combine all files in the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unicode escape'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_filenames\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdf_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdf_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
