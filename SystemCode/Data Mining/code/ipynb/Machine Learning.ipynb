{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import glob, os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import patsy\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold, GridSearchCV,learning_curve\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot_ng as pydot\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from string import printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion(src_path, extension, output_path):\n",
    "    \n",
    "    all_filenames = [i for i in glob.glob('{}*.{}'.format(src_path,extension))] \n",
    "    print('Source file(s) loaded:', all_filenames)\n",
    "    \n",
    "    #combine all files in the list \n",
    "\n",
    "    df_raw = pd.concat([pd.read_csv(f, encoding='unicode escape',skiprows=0) for f in all_filenames ]) \n",
    "    df_raw.reset_index(inplace=True) \n",
    "    df_raw = df_raw.drop(columns=['index','Unnamed: 0']) \n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(df,title_key,drop_title_key,drop_cat_key):\n",
    "    \n",
    "    #remove duplicate based on Job ID\n",
    "    job_clean = df.drop_duplicates(subset='Job_Id', keep='first')\n",
    "    #drop Salary_Type due to only one unique value 'Monthly'\n",
    "    job_clean = job_clean.drop(columns='Salary_Type')\n",
    "    #remove job without title\n",
    "    job_no_title = job_clean['Job_Title'] == ''\n",
    "    job_clean = job_clean[~job_no_title]\n",
    "    #remove row with all NaN value\n",
    "    job_clean[job_clean.isnull().any(axis=1)]\n",
    "    job_clean = job_clean.dropna()\n",
    "    \n",
    "    #perform data cleaning on every row and columms\n",
    "    clean_list = \"(\\[|\\]|b'|Requirements|'|amp;|xa0|\\\\\\|xe2x80x93|\\\\n|div class=|div class=|span class=|dib|lh-solid|/span|f5-5 i fw4 gray|f5 fw4 ph1|<|>|/div|\\\")\"\n",
    "    for col in job_clean.columns.difference(['Requirements']):\n",
    "        job_clean[col]=job_clean[col].str.replace(clean_list, \"\")\n",
    "\n",
    "    #space remain for Requirements column    \n",
    "    job_clean['Requirements']=job_clean['Requirements'].str.replace(clean_list, \" \")\n",
    "\n",
    "    #remove all non-ascii char except punctuation, digits, ascii_letters and whitespace\n",
    "    job_clean['Requirements'] = job_clean['Requirements'].apply(lambda y: ''.join(filter(lambda x: x in printable, y)))\n",
    "    \n",
    "    #further remove job with same data from all columns\n",
    "    job_clean = job_clean.drop_duplicates(subset=job_clean.columns, keep='first') \n",
    "        \n",
    "    #further filter on job title with specific keywords\n",
    "\n",
    "    key = '|'.join(title_key)\n",
    "    data_job = job_clean['Job_Title'].str.upper().str.contains(key)\n",
    "    job_clean = job_clean[data_job]\n",
    "\n",
    "    #remove job title with unwanted keywords\n",
    "    \n",
    "    key2 = '|'.join(drop_title_key)\n",
    "    non_data_job = job_clean['Job_Title'].str.upper().str.contains(key2)\n",
    "    job_clean = job_clean[~non_data_job]\n",
    "    \n",
    "    #remove job with multiple category\n",
    "    cat_list = \"(/|and)\"\n",
    "    job_clean['Category']=job_clean['Category'].str.replace(cat_list, \",\")\n",
    "    job_clean['Cat_num'] = job_clean['Category'].str.count(',')\n",
    "    \n",
    "    multiple_cat = job_clean['Cat_num']>5\n",
    "    job_clean = job_clean[~multiple_cat]\n",
    "    job_clean = job_clean.drop(columns='Cat_num')\n",
    "    \n",
    "    #remove job with no or multiple seniority\n",
    "    senior_rule = (job_clean['Seniority'].str.count(',')>=1) | (job_clean['Seniority']=='')\n",
    "    job_clean = job_clean[~senior_rule]\n",
    "   \n",
    "    #remove job cat with specific keywords\n",
    "\n",
    "    key3 = '|'.join(drop_cat_key)\n",
    "    rare_cat = job_clean['Category'].str.upper().str.contains(key3)\n",
    "    job_clean = job_clean[~rare_cat]\n",
    "    \n",
    "    #remove row without salary\n",
    "    no_salary = job_clean['Salary_Range'].str.contains('Salary undisclosed')\n",
    "    df_salary = job_clean[~no_salary]\n",
    "    df_no_salary = job_clean[no_salary]\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "    \n",
    "    req_empty = []\n",
    "\n",
    "    for i in range (len(df_salary)):\n",
    "    \n",
    "        if((len(df_salary['Requirements'][i]))<5):\n",
    "            req_empty.append(i)\n",
    "           \n",
    "    #clean & remove row without requirements\n",
    "    df_salary['Requirements']=df_salary['Requirements'].str.replace('(\\n)', \"\")\n",
    "    df_salary = df_salary.drop(req_empty)\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "\n",
    "    return df_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define list of filters\n",
    "\n",
    "title_key = ['DATA', 'MACHINE','ANALYST','MACHINE LEARNING','ANALYTICS', \"SCIENCE\", '4.0','APPLICATION', 'DEEP LEARNING',\n",
    "             'RESEARCH','NLP', 'ARTIFICIAL', \"INTELLIGENT\", 'AI', 'SCIENTIST','SYSTEM','Industry', 'IOT', 'FINANCE', 'FINTECH', \n",
    "             'SOFTWARE', 'ENGINEER', 'ENGINEERING','PROFESSOR','BUSINESS', 'DEVELOPER', 'INDUSTRIAL','AUTOMATION', 'CLOUD',\n",
    "             'SOLUTION','ARCHITECT', 'MANAGER','VP','PRESIDENT', 'TECHNOLOGY', 'SPECIALIST', 'TECHNICAL','LEAD','TECHNOLOGIST']\n",
    "\n",
    "unwanted_title_key = ['PHYSIOTHERAPIST','ACCOUNT','AUDIT','COUNSEL','EXECUTIVE','SALES','GENERAL','MARKET','ELECTRICAL','BUSINESS',\n",
    "                      'ADMIN','CUSTOMER','OFFICER','OPERATION', 'MECHANICAL','CHEMICAL','COORDINATOR','LECTURER','TECHNICIAN']\n",
    "\n",
    "unwanted_cat_key = ['HUMAN','SOCIAL','THERAPY','TAXATION','CUSTOMER','INTERIOR', 'ADMIN','BUILDING', 'SECRETARIAL','INVESTIGATION', \n",
    "                'AUDITING', 'ENVIRONMENT','SALES', 'MARKETING','ADVERTISING','CONSTRUCTION', 'DESIGN','LEGAL','HOSPITALITY',\n",
    "                'PROFESSIONAL']\n",
    "\n",
    "cat_list = ['Information Technology', 'Telecommunications', 'Engineering','Sciences', 'Finance','Healthcare','Management',\n",
    "            'Consulting','Logistics', 'Civil', 'Others']\n",
    "\n",
    "edu_list = ['phd','doctor','master','degree','computer science','engineering','statistic','math','computer engineering',\n",
    "            'business','ph.d']\n",
    "\n",
    "skills_list = ['python','java','scala','hadoop','sql','spark','tensorflow','scikit','linux','pytorch','theano','caffe','Matlab',\n",
    "               'perl','deep learning','nlp','apache','mapreduce','aws','azure','container','kafka','cassandra', 'c\\++' ,'julia',\n",
    "               'jupyter','nltk','tableau','power bi','sas','pandas','git','hive','impala','agile','machine learning','bash',\n",
    "               'natural language','oracle','cloud','flask','golang','optimization','c#','opencv','computer vision','api','jira',\n",
    "               'unix','bash','docker','keras', 'qlik','gcp','scrum', 'airflow','.net','d3.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_feature(df, low, med, high):\n",
    "    \n",
    "    #extract salary columns due to contain multiple information\n",
    "    salary_range = df[\"Salary_Range\"].str.split(\"to\", n = 2, expand = True) \n",
    "\n",
    "    #Give columns name to the dataframe\n",
    "    salary_range = salary_range.rename({0:'Min_Salary',1:'Max_Salary'}, axis='columns')\n",
    "\n",
    "    #removed $ and , from salary \n",
    "    for col in salary_range.columns:\n",
    "        salary_range[col]=salary_range[col].str.replace('(\\$|,)', '')\n",
    "\n",
    "    #convert from ojbect to float for statistical infomation\n",
    "    salary_range['Min_Salary'] = salary_range['Min_Salary'].astype('float64')\n",
    "    salary_range['Max_Salary'] = salary_range['Max_Salary'].astype('float64')\n",
    "    \n",
    "    #concat min_max salary dataframe with salary range dataframe\n",
    "    df_salary1 = pd.concat([df, salary_range], axis=1)\n",
    "    df_salary1 = df_salary1.drop(columns='Salary_Range')  \n",
    "    \n",
    "    #create a condition to check for high outliers\n",
    "    abovemean_min = round(10*np.mean(df_salary1['Min_Salary']),0)\n",
    "    abovemean_max = round(10*np.mean(df_salary1['Max_Salary']),0)\n",
    "    \n",
    "    #convert yearly salary into monthly salary\n",
    "\n",
    "    df_salary1['Min_Salary'] = np.where((df_salary1['Min_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Min_Salary']/12),0), df_salary1['Min_Salary'])\n",
    "\n",
    "    df_salary1['Max_Salary'] = np.where((df_salary1['Max_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Max_Salary']/12),0), df_salary1['Max_Salary'])\n",
    "    \n",
    "    #drop unrealistic min and max monthly salary range (which is more than 10 times)\n",
    "    min_max_abnormal = (df_salary1['Max_Salary']>10*df_salary1['Min_Salary'])\n",
    "    df_salary1 = df_salary1[~min_max_abnormal]\n",
    "    \n",
    "    #drop job with max salary less than 2500, assuming data entry/admin/operator job\n",
    "    low_sal = ((df_salary1['Min_Salary']<=1800) | (df_salary1['Max_Salary']<=2500))\n",
    "    df_salary1 = df_salary1[~low_sal]\n",
    "    \n",
    "    #create new feature for average salary\n",
    "    df_salary1['Avg_Salary'] = (df_salary1['Min_Salary'] + df_salary1['Max_Salary']) / 2\n",
    "    \n",
    "    #drop job with outlier salary\n",
    "    salary_outlier = ((df_salary1['Avg_Salary']>20000) | (df_salary1['Avg_Salary']<3000))\n",
    "    df_salary1 = df_salary1[~salary_outlier]\n",
    "    \n",
    "    #bin salary into 3 groups:\n",
    "    #3000 to 4500 - Low\n",
    "    #4500 to 6000 - Med\n",
    "    #6000 and above - High\n",
    "\n",
    "    bins = [low, med, high, np.inf]\n",
    "    names = ['Low', 'Med', 'High']\n",
    "\n",
    "    df_salary1['Salary_range'] = pd.cut(df_salary1['Avg_Salary'], bins, labels=names)\n",
    "    df_salary1 = df_salary1.reset_index(drop=True)\n",
    "    \n",
    "    return df_salary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_type(df):\n",
    "    \n",
    "    #remove others employment type\n",
    "    type_key = ['PART TIME','TEMPORARY','INTERNSHIP','FLEXI','FREELANCE']\n",
    "    key = '|'.join(type_key)\n",
    "    non_type = df['Emp_Type'].str.upper().str.contains(key)\n",
    "    df = df[~non_type]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #consolidate employment type\n",
    "    consolidate = \"(Full Time|Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Permanent\")\n",
    "\n",
    "    consolidate = \"(Contract, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Contract\")\n",
    "\n",
    "    consolidate = \"(Contract, Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Cont_Perm\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_name(df):\n",
    "    \n",
    "    stacked = pd.DataFrame(df['Category'].str.split(',').tolist()).stack()\n",
    "    cat_count = pd.DataFrame(stacked.value_counts(), columns=['Count']).reset_index()\n",
    "    cat_count1 = []\n",
    "\n",
    "    for i in range (len(cat_count)):\n",
    "        cat_count1.append(cat_count['index'][i].lstrip())\n",
    "    \n",
    "    cat_name = list(dict.fromkeys(cat_count1))\n",
    "    \n",
    "    return cat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kie(df):\n",
    "\n",
    "    #extract only number from string\n",
    "    df['Year_Experience'] = df['Year_Experience'].str.extract('(\\d+)')\n",
    "    \n",
    "    #remove comma from cell with string\n",
    "    clean_list = \"(,|;|â||¦|®|)\"\n",
    "    for col in df.columns.difference(['Year_Experience','Min_Salary','Max_Salary','Avg_Salary']):\n",
    "        df[col]=df[col].str.replace(clean_list, \"\")\n",
    "        \n",
    "    #remove extra whitespace between string\n",
    "    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    \n",
    "    #remove row without year of experience\n",
    "    df = df.dropna(subset=['Year_Experience']).reset_index(drop=True)\n",
    "    \n",
    "    #fill NaN in year of experience with 0\n",
    "    #df['Year_Experience'] = df['Year_Experience'].fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_edu_cat(df, cat_list, edu_list):\n",
    "    \n",
    "    for cat in cat_list:\n",
    "        df[cat] = np.where(df['Category'].str.lower().str.contains(cat),1,0)\n",
    "        \n",
    "    for edu in edu_list:\n",
    "        df[edu] = np.where(df['Requirements'].str.lower().str.contains(edu),1,0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_cat(df, skills_list):\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        df[skill] = np.where(df['Requirements'].str.lower().str.contains(skill),1,0)         \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count function\n",
    "def word_count(df_col):\n",
    "\n",
    "    str_counts = 0\n",
    "    sum_str = 0\n",
    "\n",
    "    for i in range (len(df_col)):    \n",
    "        str_counts = len(df_col[i].split())\n",
    "        sum_str = sum_str + str_counts\n",
    "\n",
    "    print(sum_str)\n",
    "    \n",
    "#frequent word function\n",
    "def freq_words(word_count, features):\n",
    "\n",
    "    num_word = np.asarray(word_count.sum(axis=0)).reshape(-1)\n",
    "    most_count = num_word.argsort()[::-1]\n",
    "    key_word = pd.Series(num_word[most_count], \n",
    "                           index=features[most_count])\n",
    "\n",
    "    return key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_fil(df):\n",
    "    \n",
    "    #stop words were added to filter some generic recurring business terms.\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['regret','shortlisted', 'candidates','notified','etc', 'take', 'hands','added','able','writting',\n",
    "             'year','years','least', 'related','using', 'and', 'ability','work','skills','advantage','written'\n",
    "            'develop','good','team','design','knowledge','experience','following','areas', 'ability','and','in','to']\n",
    "    \n",
    "    #most common words for requirements\n",
    "    cvt = CountVectorizer(lowercase=True, strip_accents='unicode',max_features=5000, min_df=1, max_df=0.9,\n",
    "                          stop_words=stop, ngram_range=(1,2))\n",
    "    vect_word = cvt.fit_transform(df['Requirements'])\n",
    "    features = np.array(cvt.get_feature_names()) \n",
    "\n",
    "    key_word = freq_words(vect_word, features)\n",
    "    \n",
    "    #update stop_word with common words\n",
    "    new_stop = key_word[key_word<5].index\n",
    "    stop.extend(new_stop)\n",
    "    \n",
    "    #number of word found in Requirements column before clean\n",
    "    print('Number of words before filter:')\n",
    "    word_count(df['Requirements'])\n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    df['Requirements'] = df['Requirements'].str.replace(pat, \" \")\n",
    "    df['Requirements'] = df['Requirements'].map(lambda x: x.strip())\n",
    "    df['Requirements'] = df['Requirements'].replace({' +':\" \"},regex=True)\n",
    "    \n",
    "    print('Number of words after filter:')\n",
    "    word_count(df['Requirements'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_target(data_df):\n",
    "    \n",
    "    #Dummified Seniority columns to use as predictor features\n",
    "    seniority_cat=data_df['Seniority'].str.get_dummies()\n",
    "    emp_cat=data_df['Emp_Type'].str.get_dummies()\n",
    "    df = pd.concat([data_df, seniority_cat, emp_cat], axis=1)\n",
    "\n",
    "    #CountVectorizer job requirements columns\n",
    "\n",
    "    stop_ml = stopwords.words('english')\n",
    "\n",
    "    stop_ml += ['possess','work','back','ability','communication', 'participate', 'like', 'tools','distributed',\n",
    "                'contribute','proven','engage','understanding','excellent', 'teams','experienced', 'familiarity',\n",
    "                'partners', 'study', 'well','preferably','user','field','experience','english', 'level','sets','stakeholders',\n",
    "                'delivery','implementation','relevant','state','exposure','record','problems','define','open','subject',\n",
    "                'proficient','understand','proficiency','efficiency','build', 'ideas','basic', 'technical','life','help']\n",
    "\n",
    "    cvec = CountVectorizer(lowercase=True, strip_accents='unicode',\n",
    "                           max_features=500, min_df=10, max_df=0.6, \n",
    "                           stop_words=stop_ml,ngram_range=(1,2))\n",
    "    cvec.fit(df['Requirements'])\n",
    "\n",
    "    #creating predictor and target dataset\n",
    "    model_data = df.drop(columns=['Job_Title','Company','Seniority','Category','Min_Salary',\n",
    "                                   'Max_Salary','Emp_Type','Avg_Salary', 'Job_Id', 'Date_Posted'])\n",
    "\n",
    "    nlp = pd.DataFrame(cvec.transform(model_data['Requirements']).todense(),columns=cvec.get_feature_names())\n",
    "\n",
    "    senior_nlp = pd.concat([model_data, nlp], axis=1)\n",
    "\n",
    "    X = senior_nlp.drop(columns=['Salary_range','Requirements'])\n",
    "    y = senior_nlp['Salary_range'].values\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(X, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    dtc = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    dtc_nlp = dtc.fit(X , y)\n",
    "    \n",
    "    return dtc_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(model, X, y, output_path):\n",
    "    \n",
    "    print('classficication report: \\n')\n",
    "    print(classification_report(y,model.predict(X),target_names=[\"Low\", \"Med\", \"High\"]))\n",
    "    print('\\n')\n",
    "\n",
    "    print('confusion matrix: \\n')\n",
    "    print(pd.DataFrame(confusion_matrix(y,model.predict(X)),\n",
    "                 index=['Actual Low','Actual Med', 'Actual High'],\n",
    "                 columns=['Pred Low','Pred Med','Pred High']))\n",
    "    print('\\n')\n",
    "\n",
    "    features = np.array(X.columns)\n",
    "    dt_coefs = pd.DataFrame({'coef':model.feature_importances_, 'abs coef':abs(model.feature_importances_)},index=features)\n",
    "    dt_coefs = dt_coefs.sort_values('coef',ascending=False)\n",
    "    dt_coefs.head(15)\n",
    "    print('Feature Importances\\n')\n",
    "    print(dt_coefs.head(14))\n",
    "    print('\\n')\n",
    "    \n",
    "    #this function is use to export decision tree rules\n",
    "    tree_rules = export_text(model, feature_names=list(X))\n",
    "    \n",
    "    with open(\"{}Tree_rules.txt\".format(output_path), \"w\") as text_file:\n",
    "        print(tree_rules, file=text_file)\n",
    "    \n",
    "    print('Decision tree rules: ')\n",
    "    print(tree_rules)\n",
    "    print('\\n')\n",
    "    print('Tree_rules.txt saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is use to print Decision Tree into pdf file\n",
    "def output_pdf(model, X_nlp):\n",
    "\n",
    "    #add graphviz to path\n",
    "    os.environ['PATH'] = os.environ['PATH']+';'+os.environ['CONDA_PREFIX']+r\"\\Library\\bin\\graphviz\"\n",
    "\n",
    "    feature_names = X_nlp.columns\n",
    "    class_name = dtc_nlp.classes_.astype(str)\n",
    "    \n",
    "    dot_data = StringIO()\n",
    "    tree.export_graphviz(model, out_file=dot_data,\n",
    "                         feature_names=feature_names,\n",
    "                         class_names=class_name,\n",
    "                         filled=True, rounded=True,\n",
    "                         special_characters=True,\n",
    "                          node_ids=1,)\n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    graph.write_pdf(\"{}Decision_Tree.pdf\".format(output_path))\n",
    "    print('Decision Tree diagram saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    src_path = '../data_src/' #web-crawled data source path\n",
    "    extension = 'csv' #data source format\n",
    "    output_path = '../output/' #database output path\n",
    "\n",
    "    #define salary range\n",
    "    low = 3000\n",
    "    med = 4500\n",
    "    high = 6000\n",
    "    \n",
    "    #data ingestion and data cleaning\n",
    "\n",
    "    df_raw = data_ingestion(src_path, extension, output_path)\n",
    "    df = data_clean(df_raw, title_key, unwanted_title_key, unwanted_cat_key)\n",
    "\n",
    "    #feature engineering\n",
    "    df1 = salary_feature(df, low, med, high)\n",
    "    df1 = emp_type(df1)\n",
    "    df1 = clean_kie(df1)\n",
    "    df1 = job_edu_cat(df1, cat_list, edu_list)\n",
    "    df1 = skill_cat(df1, skills_list)\n",
    "    data_df = df1.copy()\n",
    "    #data_df = stop_word_fil(df1)    \n",
    "    \n",
    "    X, y = pred_target(data_df)\n",
    "    model = model_train(X, y)\n",
    "    print_report(model, X, y, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file(s) loaded: ['../data_src\\\\artificial intelligence_data.csv', '../data_src\\\\chiropractor_data.csv', '../data_src\\\\Data Science_data.csv', '../data_src\\\\Data Science_data1.csv', '../data_src\\\\Machine Learning_data.csv', '../data_src\\\\Machine Learning_data1.csv']\n",
      "classficication report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.81      0.97      0.88       231\n",
      "         Med       0.72      0.62      0.67        37\n",
      "        High       0.80      0.44      0.57        89\n",
      "\n",
      "    accuracy                           0.80       357\n",
      "   macro avg       0.78      0.68      0.71       357\n",
      "weighted avg       0.80      0.80      0.78       357\n",
      "\n",
      "\n",
      "\n",
      "confusion matrix: \n",
      "\n",
      "             Pred Low  Pred Med  Pred High\n",
      "Actual Low        224         3          4\n",
      "Actual Med          8        23          6\n",
      "Actual High        44         6         39\n",
      "\n",
      "\n",
      "Feature Importances\n",
      "\n",
      "                     coef  abs coef\n",
      "Year_Experience  0.433057  0.433057\n",
      "java scala       0.093971  0.093971\n",
      "engineering      0.076283  0.076283\n",
      "phd              0.067935  0.067935\n",
      "design           0.050304  0.050304\n",
      "statistical      0.047727  0.047727\n",
      "architecture     0.047501  0.047501\n",
      "Executive        0.043291  0.043291\n",
      "master           0.043003  0.043003\n",
      "perl             0.023512  0.023512\n",
      "implementations  0.021084  0.021084\n",
      "optimization     0.019473  0.019473\n",
      "automation       0.017714  0.017714\n",
      "python           0.015145  0.015145\n",
      "\n",
      "\n",
      "Decision tree rules: \n",
      "|--- Year_Experience <= 3.50\n",
      "|   |--- java scala <= 0.50\n",
      "|   |   |--- phd <= 0.50\n",
      "|   |   |   |--- Year_Experience <= 1.50\n",
      "|   |   |   |   |--- Executive <= 0.50\n",
      "|   |   |   |   |   |--- class: Low\n",
      "|   |   |   |   |--- Executive >  0.50\n",
      "|   |   |   |   |   |--- class: Med\n",
      "|   |   |   |--- Year_Experience >  1.50\n",
      "|   |   |   |   |--- architecture <= 0.50\n",
      "|   |   |   |   |   |--- class: High\n",
      "|   |   |   |   |--- architecture >  0.50\n",
      "|   |   |   |   |   |--- class: Med\n",
      "|   |   |--- phd >  0.50\n",
      "|   |   |   |--- engineering <= 0.50\n",
      "|   |   |   |   |--- statistical <= 0.50\n",
      "|   |   |   |   |   |--- class: Med\n",
      "|   |   |   |   |--- statistical >  0.50\n",
      "|   |   |   |   |   |--- class: High\n",
      "|   |   |   |--- engineering >  0.50\n",
      "|   |   |   |   |--- Year_Experience <= 1.50\n",
      "|   |   |   |   |   |--- class: High\n",
      "|   |   |   |   |--- Year_Experience >  1.50\n",
      "|   |   |   |   |   |--- class: High\n",
      "|   |--- java scala >  0.50\n",
      "|   |   |--- class: High\n",
      "|--- Year_Experience >  3.50\n",
      "|   |--- master <= 0.50\n",
      "|   |   |--- perl <= 0.50\n",
      "|   |   |   |--- implementations <= 1.50\n",
      "|   |   |   |   |--- automation <= 0.50\n",
      "|   |   |   |   |   |--- class: High\n",
      "|   |   |   |   |--- automation >  0.50\n",
      "|   |   |   |   |   |--- class: High\n",
      "|   |   |   |--- implementations >  1.50\n",
      "|   |   |   |   |--- class: Low\n",
      "|   |   |--- perl >  0.50\n",
      "|   |   |   |--- python <= 0.50\n",
      "|   |   |   |   |--- class: High\n",
      "|   |   |   |--- python >  0.50\n",
      "|   |   |   |   |--- class: Med\n",
      "|   |--- master >  0.50\n",
      "|   |   |--- design <= 0.50\n",
      "|   |   |   |--- optimization <= 0.50\n",
      "|   |   |   |   |--- class: High\n",
      "|   |   |   |--- optimization >  0.50\n",
      "|   |   |   |   |--- class: Low\n",
      "|   |   |--- design >  0.50\n",
      "|   |   |   |--- Year_Experience <= 7.50\n",
      "|   |   |   |   |--- class: Med\n",
      "|   |   |   |--- Year_Experience >  7.50\n",
      "|   |   |   |   |--- class: High\n",
      "\n",
      "\n",
      "\n",
      "Tree_rules.txt saved\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
