{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import patsy\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold, GridSearchCV,learning_curve\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from string import printable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data_src\\\\AI_26Mar2020.csv', '../data_src\\\\DS_27Mar2020a.csv', '../data_src\\\\DS_27Mar2020b.csv', '../data_src\\\\ML_27Mar2020a.csv', '../data_src\\\\ML_27Mar2020b.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "\n",
    "extension = 'csv' \n",
    "src_path = '../data_src/'\n",
    "output_path = '../output/'\n",
    "\n",
    "all_filenames = [i for i in glob.glob('{}*.{}'.format(src_path,extension))] \n",
    "print(all_filenames)\n",
    "\n",
    "#combine all files in the list \n",
    "\n",
    "df_raw = pd.concat([pd.read_csv(f, encoding='unicode escape',skiprows=0) for f in all_filenames ]) \n",
    "df_raw.reset_index(inplace=True) \n",
    "df_raw = df_raw.drop(columns=['index','Unnamed: 0']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(df):\n",
    "    \n",
    "    #remove duplicate based on Job ID\n",
    "    job_clean = df_raw.drop_duplicates(subset='Job_Id', keep='first')\n",
    "    #drop Salary_Type due to only one unique value 'Monthly'\n",
    "    job_clean = job_clean.drop(columns='Salary_Type')\n",
    "    #remove job without title\n",
    "    job_no_title = job_clean['Job_Title'] == ''\n",
    "    job_clean = job_clean[~job_no_title]\n",
    "    #remove row with all NaN value\n",
    "    job_clean[job_clean.isnull().any(axis=1)]\n",
    "    job_clean = job_clean.dropna()\n",
    "    \n",
    "    #perform data cleaning on every row and columms\n",
    "    clean_list = \"(\\[|\\]|b'|Requirements|'|amp;|xa0|\\\\\\|xe2x80x93|\\\\n|div class=|div class=|span class=|dib|lh-solid|/span|f5-5 i fw4 gray|f5 fw4 ph1|<|>|/div|\\\")\"\n",
    "    for col in job_clean.columns.difference(['Requirements']):\n",
    "        job_clean[col]=job_clean[col].str.replace(clean_list, \"\")\n",
    "\n",
    "    #space remain for Requirements column    \n",
    "    job_clean['Requirements']=job_clean['Requirements'].str.replace(clean_list, \" \")\n",
    "\n",
    "    #remove all non-ascii char except punctuation, digits, ascii_letters and whitespace\n",
    "    job_clean['Requirements'] = job_clean['Requirements'].apply(lambda y: ''.join(filter(lambda x: x in printable, y)))\n",
    "    \n",
    "    #further remove job with same data from all columns\n",
    "    job_clean = job_clean.drop_duplicates(subset=job_clean.columns, keep='first') \n",
    "        \n",
    "    #further filter on job title with specific keywords\n",
    "    title_key = ['DATA', 'MACHINE','ANALYST','MACHINE LEARNING','ANALYTICS', \"SCIENCE\", '4.0','APPLICATION'\n",
    "             'DEEP LEARNING','RESEARCH','NLP', 'ARTIFICIAL', \"INTELLIGENT\", 'AI', 'SCIENTIST','SYSTEM'\n",
    "             'Industry', 'IOT', 'FINANCE', 'FINTECH', 'SOFTWARE', 'ENGINEER', 'ENGINEERING','PROFESSOR'\n",
    "             'BUSINESS', 'DEVELOPER', 'INDUSTRIAL','AUTOMATION', 'CLOUD','SOLUTION','ARCHITECT',\n",
    "             'MANAGER','VP','PRESIDENT', 'TECHNOLOGY', 'SPECIALIST', 'TECHNICAL','LEAD','TECHNOLOGIST']\n",
    "    key = '|'.join(title_key)\n",
    "    data_job = job_clean['Job_Title'].str.upper().str.contains(key)\n",
    "    job_clean = job_clean[data_job]\n",
    "\n",
    "    #remove job title with unwanted keywords\n",
    "    title_key = ['PHYSIOTHERAPIST','ACCOUNT','AUDIT','COUNSEL','EXECUTIVE','SALES','GENERAL','MARKET',\n",
    "                 'ELECTRICAL','BUSINESS','ADMIN','CUSTOMER','OFFICER','OPERATION', 'MECHANICAL','CHEMICAL',\n",
    "                 'COORDINATOR','LECTURER','TECHNICIAN']\n",
    "    key = '|'.join(title_key)\n",
    "    non_data_job = job_clean['Job_Title'].str.upper().str.contains(key)\n",
    "    job_clean = job_clean[~non_data_job]\n",
    "    \n",
    "    #remove job with multiple category\n",
    "    cat_list = \"(/|and)\"\n",
    "    job_clean['Category']=job_clean['Category'].str.replace(cat_list, \",\")\n",
    "    job_clean['Cat_num'] = job_clean['Category'].str.count(',')\n",
    "    \n",
    "    multiple_cat = job_clean['Cat_num']>5\n",
    "    job_clean = job_clean[~multiple_cat]\n",
    "    job_clean = job_clean.drop(columns='Cat_num')\n",
    "    \n",
    "    #remove job with no or multiple seniority\n",
    "    senior_rule = (job_clean['Seniority'].str.count(',')>=1) | (job_clean['Seniority']=='')\n",
    "    job_clean = job_clean[~senior_rule]\n",
    "\n",
    "    \n",
    "    #remove job cat with specific keywords\n",
    "    rare_cat_key = ['HUMAN','SOCIAL','THERAPY','TAXATION','CUSTOMER','INTERIOR', 'ADMIN','BUILDING',\n",
    "                    'SECRETARIAL','INVESTIGATION', 'AUDITING', 'ENVIRONMENT','SALES', 'MARKETING',\n",
    "                    'ADVERTISING','CONSTRUCTION', 'DESIGN','LEGAL','HOSPITALITY','PROFESSIONAL']\n",
    "    key = '|'.join(rare_cat_key)\n",
    "    rare_cat = job_clean['Category'].str.upper().str.contains(key)\n",
    "    job_clean = job_clean[~rare_cat]\n",
    "    \n",
    "    #remove row without salary\n",
    "    no_salary = job_clean['Salary_Range'].str.contains('Salary undisclosed')\n",
    "    df_salary = job_clean[~no_salary]\n",
    "    df_no_salary = job_clean[no_salary]\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "    \n",
    "    req_empty = []\n",
    "\n",
    "    for i in range (len(df_salary)):\n",
    "    \n",
    "        if((len(df_salary['Requirements'][i]))<5):\n",
    "            req_empty.append(i)\n",
    "           \n",
    "    #clean & remove row without requirements\n",
    "    df_salary['Requirements']=df_salary['Requirements'].str.replace('(\\n)', \"\")\n",
    "    df_salary = df_salary.drop(req_empty)\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "\n",
    "    return df_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_clean(df_raw)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_feature(df):\n",
    "    \n",
    "    #extract salary columns due to contain multiple information\n",
    "    salary_range = df[\"Salary_Range\"].str.split(\"to\", n = 2, expand = True) \n",
    "\n",
    "    #Give columns name to the dataframe\n",
    "    salary_range = salary_range.rename({0:'Min_Salary',1:'Max_Salary'}, axis='columns')\n",
    "\n",
    "    #removed $ and , from salary \n",
    "    for col in salary_range.columns:\n",
    "        salary_range[col]=salary_range[col].str.replace('(\\$|,)', '')\n",
    "\n",
    "    #convert from ojbect to float for statistical infomation\n",
    "    salary_range['Min_Salary'] = salary_range['Min_Salary'].astype('float64')\n",
    "    salary_range['Max_Salary'] = salary_range['Max_Salary'].astype('float64')\n",
    "    \n",
    "    #concat min_max salary dataframe with salary range dataframe\n",
    "    df_salary1 = pd.concat([df, salary_range], axis=1)\n",
    "    df_salary1 = df_salary1.drop(columns='Salary_Range')  \n",
    "    \n",
    "    #create a condition to check for high outliers\n",
    "    abovemean_min = round(10*np.mean(df_salary1['Min_Salary']),0)\n",
    "    abovemean_max = round(10*np.mean(df_salary1['Max_Salary']),0)\n",
    "    \n",
    "    #convert yearly salary into monthly salary\n",
    "\n",
    "    df_salary1['Min_Salary'] = np.where((df_salary1['Min_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Min_Salary']/12),0), df_salary1['Min_Salary'])\n",
    "\n",
    "    df_salary1['Max_Salary'] = np.where((df_salary1['Max_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Max_Salary']/12),0), df_salary1['Max_Salary'])\n",
    "    \n",
    "    #drop unrealistic min and max monthly salary range (which is more than 10 times)\n",
    "    min_max_abnormal = (df_salary1['Max_Salary']>10*df_salary1['Min_Salary'])\n",
    "    df_salary1 = df_salary1[~min_max_abnormal]\n",
    "    \n",
    "    #drop job with max salary less than 2500, assuming data entry/admin/operator job\n",
    "    low_sal = ((df_salary1['Min_Salary']<=1800) | (df_salary1['Max_Salary']<=2500))\n",
    "    df_salary1 = df_salary1[~low_sal]\n",
    "    \n",
    "    #create new feature for average salary\n",
    "    df_salary1['Avg_Salary'] = (df_salary1['Min_Salary'] + df_salary1['Max_Salary']) / 2\n",
    "    \n",
    "    #drop job with outlier salary\n",
    "    salary_outlier = ((df_salary1['Avg_Salary']>20000) | (df_salary1['Avg_Salary']<3000))\n",
    "    df_salary1 = df_salary1[~salary_outlier]\n",
    "    \n",
    "    #bin salary into 4 groups:\n",
    "    #3000 to 5500 - Low\n",
    "    #5500 to 8000 - Med\n",
    "    #8000 and above - High\n",
    "\n",
    "    bins = [3000, 5500, 8000, np.inf]\n",
    "    names = ['Low', 'Med', 'High']\n",
    "\n",
    "    df_salary1['Salary_range'] = pd.cut(df_salary1['Avg_Salary'], bins, labels=names)\n",
    "    df_salary1 = df_salary1.reset_index(drop=True)\n",
    "    \n",
    "    return df_salary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = salary_feature(df)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_type(df):\n",
    "\n",
    "    \n",
    "    #remove others employment type\n",
    "    type_key = ['PART TIME','TEMPORARY','INTERNSHIP','FLEXI','FREELANCE']\n",
    "    key = '|'.join(type_key)\n",
    "    non_type = df['Emp_Type'].str.upper().str.contains(key)\n",
    "    df = df[~non_type]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #consolidate employment type\n",
    "    consolidate = \"(Full Time|Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Permanent\")\n",
    "\n",
    "    consolidate = \"(Contract, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Contract\")\n",
    "\n",
    "    consolidate = \"(Contract, Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Cont_Perm\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(453, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = emp_type(df1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seniority(df):\n",
    "    \n",
    "    #consolidate seniority from 9 groups to 4 groups\n",
    "    \n",
    "    df['Seniority'] = np.where((df['Seniority'] == 'Fresh/entry level') | (df['Seniority'] == 'Non-executive') | (df['Seniority'] == 'Junior Executive'),\n",
    "                                 'Jr Executive', df['Seniority'])\n",
    "    df['Seniority'] = np.where((df['Seniority'] == 'Executive') | (df['Seniority'] == 'Senior Executive'),\n",
    "                                 'Sr Executive', df['Seniority'])\n",
    "    df['Seniority'] = np.where((df['Seniority'] == 'Manager') | (df['Seniority'] == 'Middle Management') | (df['Seniority'] == 'Senior Management'),\n",
    "                                 'Management', df['Seniority'])   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(453, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = seniority(df1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_name(df):\n",
    "    \n",
    "    stacked = pd.DataFrame(df['Category'].str.split(',').tolist()).stack()\n",
    "    cat_count = pd.DataFrame(stacked.value_counts(), columns=['Count']).reset_index()\n",
    "    cat_count1 = []\n",
    "\n",
    "    for i in range (len(cat_count)):\n",
    "        cat_count1.append(cat_count['index'][i].lstrip())\n",
    "    \n",
    "    cat_name = list(dict.fromkeys(cat_count1))\n",
    "    return cat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R&D', 'Laboratory ', 'Information Technology', 'Sciences ', 'Engineering', 'Finance', 'Banking ', 'Civil Service', 'Public ', 'Consulting', 'Telecommunications', 'Others', 'Manufacturing', 'Pharmaceutical', 'Training', 'Education ', 'Healthcare ', 'Insurance', 'General Management', 'Supply Chain', 'Logistics ', 'Risk Management']\n"
     ]
    }
   ],
   "source": [
    "print(cat_name(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kie(df):\n",
    "\n",
    "    #extract only number from string\n",
    "    df['Year_Experience'] = df['Year_Experience'].str.extract('(\\d+)')\n",
    "    \n",
    "    #remove comma from cell with string\n",
    "    clean_list = \"(,|;|â||¦|®|)\"\n",
    "    for col in df.columns.difference(['Year_Experience','Min_Salary','Max_Salary','Avg_Salary']):\n",
    "        df[col]=df[col].str.replace(clean_list, \"\")\n",
    "        \n",
    "    #remove extra whitespace between string\n",
    "    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    \n",
    "    #fill NaN in year of experience with 0\n",
    "    df['Year_Experience'] = df['Year_Experience'].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Id</th>\n",
       "      <th>Emp_Type</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Date_Posted</th>\n",
       "      <th>Year_Experience</th>\n",
       "      <th>Seniority</th>\n",
       "      <th>Category</th>\n",
       "      <th>Requirements</th>\n",
       "      <th>Min_Salary</th>\n",
       "      <th>Max_Salary</th>\n",
       "      <th>Avg_Salary</th>\n",
       "      <th>Salary_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCF-2020-0072476</td>\n",
       "      <td>Permanent</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>LINGJACK ENGINEERING WORKS PTE LTD</td>\n",
       "      <td>Posted 26 Mar 2020</td>\n",
       "      <td>2</td>\n",
       "      <td>Sr Executive</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Perform data analysis labelling modelling and ...</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>Med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MCF-2020-0072378</td>\n",
       "      <td>Contract Permanent</td>\n",
       "      <td>Research Engineer (Deep Learning 2.0) I2R</td>\n",
       "      <td>A*STAR RESEARCH ENTITIES</td>\n",
       "      <td>Posted 26 Mar 2020</td>\n",
       "      <td>0</td>\n",
       "      <td>Jr Executive</td>\n",
       "      <td>Sciences  Laboratory  R&amp;D</td>\n",
       "      <td>Minimum Bachelor in Computer Science Statistic...</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>7800.0</td>\n",
       "      <td>5850.0</td>\n",
       "      <td>Med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Job_Id            Emp_Type  \\\n",
       "0  MCF-2020-0072476           Permanent   \n",
       "1  MCF-2020-0072378  Contract Permanent   \n",
       "\n",
       "                                   Job_Title  \\\n",
       "0                               Data Analyst   \n",
       "1  Research Engineer (Deep Learning 2.0) I2R   \n",
       "\n",
       "                              Company         Date_Posted Year_Experience  \\\n",
       "0  LINGJACK ENGINEERING WORKS PTE LTD  Posted 26 Mar 2020               2   \n",
       "1            A*STAR RESEARCH ENTITIES  Posted 26 Mar 2020               0   \n",
       "\n",
       "      Seniority                   Category  \\\n",
       "0  Sr Executive     Information Technology   \n",
       "1  Jr Executive  Sciences  Laboratory  R&D   \n",
       "\n",
       "                                        Requirements  Min_Salary  Max_Salary  \\\n",
       "0  Perform data analysis labelling modelling and ...      5000.0      7000.0   \n",
       "1  Minimum Bachelor in Computer Science Statistic...      3900.0      7800.0   \n",
       "\n",
       "   Avg_Salary Salary_range  \n",
       "0      6000.0          Med  \n",
       "1      5850.0          Med  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = clean_kie(df1)\n",
    "print(df_clean.shape)\n",
    "df_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count function\n",
    "def word_count(df_col):\n",
    "\n",
    "    str_counts = 0\n",
    "    sum_str = 0\n",
    "\n",
    "    for i in range (len(df_col)):    \n",
    "        str_counts = len(df_col[i].split())\n",
    "        sum_str = sum_str + str_counts\n",
    "\n",
    "    print(sum_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78225\n"
     ]
    }
   ],
   "source": [
    "#number of word found in Requirements column before clean\n",
    "word_count(df_clean['Requirements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(word_count, features):\n",
    "\n",
    "    num_word = np.asarray(word_count.sum(axis=0)).reshape(-1)\n",
    "    most_count = num_word.argsort()[::-1]\n",
    "    key_word = pd.Series(num_word[most_count], \n",
    "                           index=features[most_count])\n",
    "\n",
    "    return key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_fil(df):\n",
    "    \n",
    "    #stop words were added to filter some generic recurring business terms.\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['regret','shortlisted', 'candidates','notified','etc', 'take', 'hands','added','able','writting',\n",
    "             'year','years','least', 'related','using', 'and', 'ability','work','skills','advantage','written'\n",
    "            'develop','good','team','design','knowledge','experience','following','areas', 'ability','and','in','to']\n",
    "    \n",
    "    #most common words for requirements\n",
    "    cvt = CountVectorizer(lowercase=True, strip_accents='unicode',max_features=80000, min_df=1, max_df=0.9,\n",
    "                          stop_words=stop, ngram_range=(1,2))\n",
    "    vect_word = cvt.fit_transform(df['Requirements'])\n",
    "    features = np.array(cvt.get_feature_names()) \n",
    "\n",
    "    key_word = freq_words(vect_word, features)\n",
    "    \n",
    "    #update stop_word with common words\n",
    "    new_stop = key_word[key_word<5].index\n",
    "    stop.extend(new_stop)\n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    df['Requirements'] = df['Requirements'].str.replace(pat, \" \")\n",
    "    df['Requirements'] = df['Requirements'].map(lambda x: x.strip())\n",
    "    df['Requirements'] = df['Requirements'].replace({' +':\" \"},regex=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39124\n"
     ]
    }
   ],
   "source": [
    "data_df = stop_word_fil(df_clean)\n",
    "\n",
    "#number of word found in Requirements column after clean\n",
    "word_count(data_df['Requirements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output file for KIE\n",
    "\n",
    "data_df.to_csv('{}JOB_DATA_v11.csv'.format(output_path), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummified Seniority columns to use as predictor features\n",
    "seniority_cat=data_df['Seniority'].str.get_dummies()\n",
    "salary_cat=data_df['Salary_range'].str.get_dummies()\n",
    "df = pd.concat([data_df, seniority_cat, salary_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer job requirements columns\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "cvec = CountVectorizer(lowercase=True, strip_accents='unicode',\n",
    "                       max_features=1500, min_df=5, max_df=0.7, \n",
    "                       stop_words=stop,ngram_range=(2,3))\n",
    "cvec.fit(df['Requirements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating predictor and target dataset\n",
    "model_data = df.drop(columns=['Job_Title','Company','Seniority','Category','Min_Salary',\n",
    "                               'Max_Salary','Emp_Type','Avg_Salary', 'Job_Id', 'Date_Posted'])\n",
    "\n",
    "nlp = pd.DataFrame(cvec.transform(model_data['Requirements']).todense(),columns=cvec.get_feature_names())\n",
    "\n",
    "senior_nlp = pd.concat([model_data, nlp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = senior_nlp.drop(columns=['Salary_range','Requirements'])\n",
    "X_nlp = nlp\n",
    "y = senior_nlp['Salary_range'].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with dummified 'seniority' and countvectorized 'requirements'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with countvectorized 'requirements' only\n",
    "X_train_nlp, X_test_nlp, y_train_nlp, y_test_nlp = train_test_split(X_nlp, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dtc = dtc.fit(X_train , y_train)\n",
    "\n",
    "dtc1 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dtc_nlp = dtc1.fit(X_train_nlp , y_train_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,dtc.predict(X_test),target_names=[\"Low\", \"Med\", \"High\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test,dtc.predict(X_test)),\n",
    "             index=['Actual Low','Actual Med', 'Actual High'],\n",
    "             columns=['Pred Low','Pred Med','Pred High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_nlp,dtc_nlp.predict(X_test_nlp),target_names=[\"Low\", \"Med\", \"High\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test_nlp,dtc_nlp.predict(X_test_nlp)),\n",
    "             index=['Actual Low','Actual Med', 'Actual High'],\n",
    "             columns=['Pred Low','Pred Med','Pred High'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
